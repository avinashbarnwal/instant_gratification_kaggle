{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "Lasso + Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso + Gaussian Mixture ModelsÂ¶\n",
    "With this kernel I want to demonstrate how to use Gaussian mixture Models (GMM) which have the nice property to train unsupervised, so you can also use the test set. I use Graphical Lasso as an estimator for the initial value of precision matrix (= inverse Covariance) and mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>muggy-smalt-axolotl-pembus</th>\n",
       "      <th>dorky-peach-sheepdog-ordinal</th>\n",
       "      <th>slimy-seashell-cassowary-goose</th>\n",
       "      <th>snazzy-harlequin-chicken-distraction</th>\n",
       "      <th>frumpy-smalt-mau-ordinal</th>\n",
       "      <th>stealthy-beige-pinscher-golden</th>\n",
       "      <th>chummy-cream-tarantula-entropy</th>\n",
       "      <th>hazy-emerald-cuttlefish-unsorted</th>\n",
       "      <th>nerdy-indigo-wolfhound-sorted</th>\n",
       "      <th>leaky-amaranth-lizard-sorted</th>\n",
       "      <th>ugly-tangerine-chihuahua-important</th>\n",
       "      <th>shaggy-silver-indri-fimbus</th>\n",
       "      <th>flaky-chocolate-beetle-grandmaster</th>\n",
       "      <th>squirrely-harlequin-sheep-sumble</th>\n",
       "      <th>freaky-tan-angelfish-noise</th>\n",
       "      <th>lousy-plum-penguin-sumble</th>\n",
       "      <th>bluesy-rose-wallaby-discard</th>\n",
       "      <th>baggy-copper-oriole-dummy</th>\n",
       "      <th>stealthy-scarlet-hound-fepid</th>\n",
       "      <th>greasy-cinnamon-bonobo-contributor</th>\n",
       "      <th>cranky-cardinal-dogfish-ordinal</th>\n",
       "      <th>snippy-auburn-vole-learn</th>\n",
       "      <th>greasy-sepia-coral-dataset</th>\n",
       "      <th>flabby-tangerine-fowl-entropy</th>\n",
       "      <th>lousy-smalt-pinscher-dummy</th>\n",
       "      <th>bluesy-brass-chihuahua-distraction</th>\n",
       "      <th>goopy-eggplant-indri-entropy</th>\n",
       "      <th>homey-sepia-bombay-sorted</th>\n",
       "      <th>homely-ruby-bulldog-entropy</th>\n",
       "      <th>hasty-blue-sheep-contributor</th>\n",
       "      <th>blurry-wisteria-oyster-master</th>\n",
       "      <th>snoopy-auburn-dogfish-expert</th>\n",
       "      <th>stinky-maroon-blue-kernel</th>\n",
       "      <th>bumpy-amaranth-armadillo-important</th>\n",
       "      <th>slaphappy-peach-oyster-master</th>\n",
       "      <th>dorky-tomato-ragdoll-dataset</th>\n",
       "      <th>messy-mauve-wolverine-ordinal</th>\n",
       "      <th>geeky-pumpkin-moorhen-important</th>\n",
       "      <th>crabby-teal-otter-unsorted</th>\n",
       "      <th>...</th>\n",
       "      <th>beady-mauve-frog-distraction</th>\n",
       "      <th>surly-brass-maltese-ordinal</th>\n",
       "      <th>beady-asparagus-opossum-expert</th>\n",
       "      <th>beady-rust-impala-dummy</th>\n",
       "      <th>droopy-amethyst-dachshund-hint</th>\n",
       "      <th>homey-crimson-budgerigar-grandmaster</th>\n",
       "      <th>droopy-cardinal-impala-important</th>\n",
       "      <th>woozy-apricot-moose-hint</th>\n",
       "      <th>paltry-sapphire-labradoodle-dummy</th>\n",
       "      <th>crappy-carmine-eagle-entropy</th>\n",
       "      <th>greasy-magnolia-spider-grandmaster</th>\n",
       "      <th>crabby-carmine-flounder-sorted</th>\n",
       "      <th>skimpy-copper-fowl-grandmaster</th>\n",
       "      <th>hasty-seashell-woodpecker-hint</th>\n",
       "      <th>snappy-purple-bobcat-important</th>\n",
       "      <th>thirsty-carmine-corgi-ordinal</th>\n",
       "      <th>homely-auburn-reindeer-unsorted</th>\n",
       "      <th>crappy-beige-tiger-fepid</th>\n",
       "      <th>cranky-auburn-swan-novice</th>\n",
       "      <th>chewy-bistre-buzzard-expert</th>\n",
       "      <th>skinny-cyan-macaque-pembus</th>\n",
       "      <th>slimy-periwinkle-otter-expert</th>\n",
       "      <th>snazzy-burgundy-clam-novice</th>\n",
       "      <th>cozy-ochre-gorilla-gaussian</th>\n",
       "      <th>homey-sangria-wolfhound-dummy</th>\n",
       "      <th>snazzy-asparagus-hippopotamus-contributor</th>\n",
       "      <th>paltry-red-hamster-sorted</th>\n",
       "      <th>zippy-dandelion-insect-golden</th>\n",
       "      <th>baggy-coral-bandicoot-unsorted</th>\n",
       "      <th>goopy-lavender-wolverine-fimbus</th>\n",
       "      <th>wheezy-myrtle-mandrill-entropy</th>\n",
       "      <th>wiggy-lilac-lemming-sorted</th>\n",
       "      <th>gloppy-cerise-snail-contributor</th>\n",
       "      <th>woozy-silver-havanese-gaussian</th>\n",
       "      <th>jumpy-thistle-discus-sorted</th>\n",
       "      <th>muggy-turquoise-donkey-important</th>\n",
       "      <th>blurry-buff-hyena-entropy</th>\n",
       "      <th>bluesy-chocolate-kudu-fepid</th>\n",
       "      <th>gamy-white-monster-expert</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>707b395ecdcbb4dc2eabea00e4d1b179</td>\n",
       "      <td>-2.070654</td>\n",
       "      <td>1.018160</td>\n",
       "      <td>0.228643</td>\n",
       "      <td>0.857221</td>\n",
       "      <td>0.052271</td>\n",
       "      <td>0.230303</td>\n",
       "      <td>-6.385090</td>\n",
       "      <td>0.439369</td>\n",
       "      <td>-0.721946</td>\n",
       "      <td>-0.227027</td>\n",
       "      <td>0.575964</td>\n",
       "      <td>1.541908</td>\n",
       "      <td>1.745286</td>\n",
       "      <td>-0.624271</td>\n",
       "      <td>3.600958</td>\n",
       "      <td>1.176489</td>\n",
       "      <td>-0.182776</td>\n",
       "      <td>-0.228391</td>\n",
       "      <td>1.682263</td>\n",
       "      <td>-0.833236</td>\n",
       "      <td>-4.377688</td>\n",
       "      <td>-5.372410</td>\n",
       "      <td>-0.477742</td>\n",
       "      <td>-0.179005</td>\n",
       "      <td>-0.516475</td>\n",
       "      <td>0.127391</td>\n",
       "      <td>-0.857591</td>\n",
       "      <td>-0.461500</td>\n",
       "      <td>2.160303</td>\n",
       "      <td>-2.118371</td>\n",
       "      <td>0.515493</td>\n",
       "      <td>-1.201493</td>\n",
       "      <td>-0.027377</td>\n",
       "      <td>-1.154024</td>\n",
       "      <td>0.753204</td>\n",
       "      <td>-0.179651</td>\n",
       "      <td>-0.807341</td>\n",
       "      <td>-1.663626</td>\n",
       "      <td>0.893806</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.829848</td>\n",
       "      <td>2.347131</td>\n",
       "      <td>0.082462</td>\n",
       "      <td>-1.012654</td>\n",
       "      <td>0.593752</td>\n",
       "      <td>2.904654</td>\n",
       "      <td>-0.428974</td>\n",
       "      <td>-0.919979</td>\n",
       "      <td>2.849575</td>\n",
       "      <td>-0.906744</td>\n",
       "      <td>0.729459</td>\n",
       "      <td>0.386140</td>\n",
       "      <td>0.319814</td>\n",
       "      <td>-0.407682</td>\n",
       "      <td>-0.170667</td>\n",
       "      <td>-1.242919</td>\n",
       "      <td>-1.719046</td>\n",
       "      <td>-0.132395</td>\n",
       "      <td>-0.368991</td>\n",
       "      <td>-5.112553</td>\n",
       "      <td>-2.085988</td>\n",
       "      <td>-0.897257</td>\n",
       "      <td>1.080671</td>\n",
       "      <td>-0.273262</td>\n",
       "      <td>0.342824</td>\n",
       "      <td>0.640177</td>\n",
       "      <td>-0.415298</td>\n",
       "      <td>-0.483126</td>\n",
       "      <td>-0.080799</td>\n",
       "      <td>2.416224</td>\n",
       "      <td>0.351895</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>-1.542423</td>\n",
       "      <td>0.598175</td>\n",
       "      <td>0.611757</td>\n",
       "      <td>0.678772</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>-0.806677</td>\n",
       "      <td>-0.193649</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5880c03c6582a7b42248668e56b4bdec</td>\n",
       "      <td>-0.491702</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>-0.011193</td>\n",
       "      <td>1.071266</td>\n",
       "      <td>-0.346347</td>\n",
       "      <td>-0.082209</td>\n",
       "      <td>0.110579</td>\n",
       "      <td>-0.382374</td>\n",
       "      <td>-0.229620</td>\n",
       "      <td>0.783980</td>\n",
       "      <td>-1.280579</td>\n",
       "      <td>-1.003480</td>\n",
       "      <td>-7.753201</td>\n",
       "      <td>-1.320547</td>\n",
       "      <td>0.919078</td>\n",
       "      <td>-1.036068</td>\n",
       "      <td>0.030213</td>\n",
       "      <td>0.910172</td>\n",
       "      <td>-0.905345</td>\n",
       "      <td>0.646641</td>\n",
       "      <td>-0.465291</td>\n",
       "      <td>-0.531735</td>\n",
       "      <td>-0.756781</td>\n",
       "      <td>0.193724</td>\n",
       "      <td>0.224277</td>\n",
       "      <td>-0.474412</td>\n",
       "      <td>1.885805</td>\n",
       "      <td>0.205439</td>\n",
       "      <td>-6.481422</td>\n",
       "      <td>1.035620</td>\n",
       "      <td>-0.453623</td>\n",
       "      <td>0.375936</td>\n",
       "      <td>-0.320670</td>\n",
       "      <td>-0.144646</td>\n",
       "      <td>-0.220129</td>\n",
       "      <td>0.577826</td>\n",
       "      <td>-0.360512</td>\n",
       "      <td>-0.600107</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982205</td>\n",
       "      <td>-1.161978</td>\n",
       "      <td>0.532269</td>\n",
       "      <td>1.133215</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>-1.390962</td>\n",
       "      <td>0.158572</td>\n",
       "      <td>0.143794</td>\n",
       "      <td>-0.317185</td>\n",
       "      <td>1.017192</td>\n",
       "      <td>-0.395342</td>\n",
       "      <td>-0.642357</td>\n",
       "      <td>-0.627209</td>\n",
       "      <td>0.257271</td>\n",
       "      <td>-1.461564</td>\n",
       "      <td>0.325613</td>\n",
       "      <td>1.628369</td>\n",
       "      <td>0.640040</td>\n",
       "      <td>0.750735</td>\n",
       "      <td>1.164573</td>\n",
       "      <td>0.900373</td>\n",
       "      <td>0.063489</td>\n",
       "      <td>0.948158</td>\n",
       "      <td>0.273014</td>\n",
       "      <td>-1.269147</td>\n",
       "      <td>-0.251101</td>\n",
       "      <td>-2.271731</td>\n",
       "      <td>-0.044167</td>\n",
       "      <td>-0.443766</td>\n",
       "      <td>-1.144794</td>\n",
       "      <td>-0.645115</td>\n",
       "      <td>-1.246090</td>\n",
       "      <td>2.613357</td>\n",
       "      <td>-0.479664</td>\n",
       "      <td>1.581289</td>\n",
       "      <td>0.931258</td>\n",
       "      <td>0.151937</td>\n",
       "      <td>-0.766595</td>\n",
       "      <td>0.474351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4ccbcb3d13e5072ff1d9c61afe2c4f77</td>\n",
       "      <td>-1.680473</td>\n",
       "      <td>0.860529</td>\n",
       "      <td>-1.076195</td>\n",
       "      <td>0.740124</td>\n",
       "      <td>3.678445</td>\n",
       "      <td>0.288558</td>\n",
       "      <td>0.515875</td>\n",
       "      <td>0.920590</td>\n",
       "      <td>-1.223277</td>\n",
       "      <td>-1.029780</td>\n",
       "      <td>-2.203397</td>\n",
       "      <td>-7.088717</td>\n",
       "      <td>0.438218</td>\n",
       "      <td>-0.848173</td>\n",
       "      <td>1.542666</td>\n",
       "      <td>-2.166858</td>\n",
       "      <td>-0.867670</td>\n",
       "      <td>-0.980947</td>\n",
       "      <td>0.567793</td>\n",
       "      <td>1.323430</td>\n",
       "      <td>-2.076700</td>\n",
       "      <td>-0.291598</td>\n",
       "      <td>-1.564816</td>\n",
       "      <td>-8.718695</td>\n",
       "      <td>0.340144</td>\n",
       "      <td>-0.566402</td>\n",
       "      <td>0.844324</td>\n",
       "      <td>0.816421</td>\n",
       "      <td>-1.019114</td>\n",
       "      <td>-0.881431</td>\n",
       "      <td>-2.285710</td>\n",
       "      <td>-0.090958</td>\n",
       "      <td>-0.898440</td>\n",
       "      <td>-0.584417</td>\n",
       "      <td>-0.143660</td>\n",
       "      <td>-0.182084</td>\n",
       "      <td>0.798516</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>-0.347155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829467</td>\n",
       "      <td>0.588236</td>\n",
       "      <td>0.427946</td>\n",
       "      <td>-0.563037</td>\n",
       "      <td>-0.103990</td>\n",
       "      <td>-0.817698</td>\n",
       "      <td>1.251046</td>\n",
       "      <td>-0.977157</td>\n",
       "      <td>2.732600</td>\n",
       "      <td>1.997984</td>\n",
       "      <td>-0.214285</td>\n",
       "      <td>-0.389428</td>\n",
       "      <td>-1.007633</td>\n",
       "      <td>0.336435</td>\n",
       "      <td>-0.851292</td>\n",
       "      <td>-0.024184</td>\n",
       "      <td>0.455908</td>\n",
       "      <td>0.458753</td>\n",
       "      <td>-0.267230</td>\n",
       "      <td>-2.032402</td>\n",
       "      <td>0.203082</td>\n",
       "      <td>0.654107</td>\n",
       "      <td>-3.512338</td>\n",
       "      <td>-0.840937</td>\n",
       "      <td>0.519407</td>\n",
       "      <td>-0.028053</td>\n",
       "      <td>-1.621083</td>\n",
       "      <td>0.142132</td>\n",
       "      <td>1.514664</td>\n",
       "      <td>0.828815</td>\n",
       "      <td>0.516422</td>\n",
       "      <td>0.130521</td>\n",
       "      <td>-0.459210</td>\n",
       "      <td>2.028205</td>\n",
       "      <td>-0.093968</td>\n",
       "      <td>-0.218274</td>\n",
       "      <td>-0.163136</td>\n",
       "      <td>-0.870289</td>\n",
       "      <td>0.064038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e350f17a357f12a1941f0837afb7eb8d</td>\n",
       "      <td>0.183774</td>\n",
       "      <td>0.919134</td>\n",
       "      <td>-0.946958</td>\n",
       "      <td>0.918492</td>\n",
       "      <td>0.862278</td>\n",
       "      <td>1.155287</td>\n",
       "      <td>0.911106</td>\n",
       "      <td>0.562598</td>\n",
       "      <td>-1.349685</td>\n",
       "      <td>-1.182729</td>\n",
       "      <td>0.003159</td>\n",
       "      <td>-0.626847</td>\n",
       "      <td>0.368980</td>\n",
       "      <td>1.560784</td>\n",
       "      <td>0.502851</td>\n",
       "      <td>-0.108050</td>\n",
       "      <td>0.633208</td>\n",
       "      <td>-0.411502</td>\n",
       "      <td>-3.201592</td>\n",
       "      <td>-0.710612</td>\n",
       "      <td>0.786816</td>\n",
       "      <td>0.500979</td>\n",
       "      <td>-1.040048</td>\n",
       "      <td>-1.369170</td>\n",
       "      <td>0.987666</td>\n",
       "      <td>-0.681838</td>\n",
       "      <td>-0.331372</td>\n",
       "      <td>2.254289</td>\n",
       "      <td>-0.009330</td>\n",
       "      <td>2.007067</td>\n",
       "      <td>1.203750</td>\n",
       "      <td>-2.003928</td>\n",
       "      <td>-0.566088</td>\n",
       "      <td>0.223452</td>\n",
       "      <td>0.434202</td>\n",
       "      <td>-1.203766</td>\n",
       "      <td>-0.103490</td>\n",
       "      <td>0.441111</td>\n",
       "      <td>1.818458</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.231836</td>\n",
       "      <td>0.833236</td>\n",
       "      <td>-0.454226</td>\n",
       "      <td>-1.614694</td>\n",
       "      <td>0.159948</td>\n",
       "      <td>-0.150059</td>\n",
       "      <td>-1.570599</td>\n",
       "      <td>0.960839</td>\n",
       "      <td>0.102214</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.852834</td>\n",
       "      <td>-1.265608</td>\n",
       "      <td>-3.219190</td>\n",
       "      <td>0.251194</td>\n",
       "      <td>0.215861</td>\n",
       "      <td>-0.009520</td>\n",
       "      <td>1.611203</td>\n",
       "      <td>1.679806</td>\n",
       "      <td>-0.008419</td>\n",
       "      <td>0.658384</td>\n",
       "      <td>-0.132437</td>\n",
       "      <td>-1.466823</td>\n",
       "      <td>-1.577080</td>\n",
       "      <td>-0.800346</td>\n",
       "      <td>1.960795</td>\n",
       "      <td>-4.042900</td>\n",
       "      <td>1.722143</td>\n",
       "      <td>-0.261888</td>\n",
       "      <td>-1.145005</td>\n",
       "      <td>-1.864582</td>\n",
       "      <td>-1.168967</td>\n",
       "      <td>1.385089</td>\n",
       "      <td>-0.353028</td>\n",
       "      <td>3.316150</td>\n",
       "      <td>-0.524087</td>\n",
       "      <td>-0.794327</td>\n",
       "      <td>3.936365</td>\n",
       "      <td>0.682989</td>\n",
       "      <td>-2.521211</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a8f910ea6075b6376af079055965ff68</td>\n",
       "      <td>-0.203933</td>\n",
       "      <td>-0.177252</td>\n",
       "      <td>0.368074</td>\n",
       "      <td>-0.701320</td>\n",
       "      <td>-1.104391</td>\n",
       "      <td>0.735760</td>\n",
       "      <td>0.894273</td>\n",
       "      <td>-1.375826</td>\n",
       "      <td>-5.144946</td>\n",
       "      <td>-2.048711</td>\n",
       "      <td>0.629773</td>\n",
       "      <td>-4.252669</td>\n",
       "      <td>-0.087420</td>\n",
       "      <td>-0.794367</td>\n",
       "      <td>-1.063963</td>\n",
       "      <td>0.115997</td>\n",
       "      <td>0.895180</td>\n",
       "      <td>3.184848</td>\n",
       "      <td>2.057840</td>\n",
       "      <td>-0.950821</td>\n",
       "      <td>0.961059</td>\n",
       "      <td>-1.837828</td>\n",
       "      <td>-0.437156</td>\n",
       "      <td>-0.828433</td>\n",
       "      <td>0.373747</td>\n",
       "      <td>-0.099787</td>\n",
       "      <td>-0.976280</td>\n",
       "      <td>-0.165921</td>\n",
       "      <td>3.297221</td>\n",
       "      <td>3.914132</td>\n",
       "      <td>-4.971376</td>\n",
       "      <td>-0.286520</td>\n",
       "      <td>-0.160133</td>\n",
       "      <td>-3.301453</td>\n",
       "      <td>-1.021032</td>\n",
       "      <td>-0.562744</td>\n",
       "      <td>0.574065</td>\n",
       "      <td>-0.368194</td>\n",
       "      <td>-0.507458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178099</td>\n",
       "      <td>-0.410396</td>\n",
       "      <td>-1.184236</td>\n",
       "      <td>1.681727</td>\n",
       "      <td>0.589606</td>\n",
       "      <td>0.064222</td>\n",
       "      <td>0.258885</td>\n",
       "      <td>0.560241</td>\n",
       "      <td>-1.545597</td>\n",
       "      <td>0.822283</td>\n",
       "      <td>1.518209</td>\n",
       "      <td>0.460143</td>\n",
       "      <td>0.822488</td>\n",
       "      <td>1.362718</td>\n",
       "      <td>0.218560</td>\n",
       "      <td>-1.038514</td>\n",
       "      <td>1.000763</td>\n",
       "      <td>-0.975878</td>\n",
       "      <td>-0.551268</td>\n",
       "      <td>-0.133044</td>\n",
       "      <td>-0.393092</td>\n",
       "      <td>1.236473</td>\n",
       "      <td>1.657100</td>\n",
       "      <td>0.833020</td>\n",
       "      <td>0.665379</td>\n",
       "      <td>-0.900025</td>\n",
       "      <td>0.291908</td>\n",
       "      <td>0.482727</td>\n",
       "      <td>0.552399</td>\n",
       "      <td>0.970496</td>\n",
       "      <td>-0.279168</td>\n",
       "      <td>1.544356</td>\n",
       "      <td>2.959727</td>\n",
       "      <td>1.641201</td>\n",
       "      <td>-0.130818</td>\n",
       "      <td>-0.264292</td>\n",
       "      <td>-0.748668</td>\n",
       "      <td>0.964218</td>\n",
       "      <td>0.087079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id   ...    target\n",
       "0  707b395ecdcbb4dc2eabea00e4d1b179   ...         0\n",
       "1  5880c03c6582a7b42248668e56b4bdec   ...         0\n",
       "2  4ccbcb3d13e5072ff1d9c61afe2c4f77   ...         1\n",
       "3  e350f17a357f12a1941f0837afb7eb8d   ...         0\n",
       "4  a8f910ea6075b6376af079055965ff68   ...         0\n",
       "\n",
       "[5 rows x 258 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, os\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import sympy \n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import GraphicalLasso\n",
    "\n",
    "def get_mean_cov(x,y):\n",
    "    model = GraphicalLasso()\n",
    "    ones = (y==1).astype(bool)\n",
    "    x2 = x[ones]\n",
    "    model.fit(x2)\n",
    "    p1 = model.precision_\n",
    "    m1 = model.location_\n",
    "    \n",
    "    onesb = (y==0).astype(bool)\n",
    "    x2b = x[onesb]\n",
    "    model.fit(x2b)\n",
    "    p2 = model.precision_\n",
    "    m2 = model.location_\n",
    "    \n",
    "    ms = np.stack([m1,m2])\n",
    "    ps = np.stack([p1,p2])\n",
    "    return ms,ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate cov and mean from Lasso and predict with GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/512 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.327e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 13%|ââ        | 68/512 [02:40<16:42,  2.26s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.271e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 16%|ââ        | 80/512 [03:08<16:34,  2.30s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.834e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 19%|ââ        | 97/512 [03:48<15:27,  2.23s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.377e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 65%|âââââââ   | 332/512 [12:54<07:04,  2.36s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.144e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 73%|ââââââââ  | 375/512 [14:33<05:10,  2.26s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: -2.243e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: -1.275e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: -2.746e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: -1.973e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      " 90%|âââââââââ | 463/512 [17:54<01:51,  2.27s/it]/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.310e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.542e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.152e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 1.404e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 2.338e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/covariance/graph_lasso_.py:265: ConvergenceWarning: graphical_lasso: did not converge after 100 iteration: dual gap: 2.551e-04\n",
      "  % (max_iter, d_gap), ConvergenceWarning)\n",
      "100%|ââââââââââ| 512/512 [19:53<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA scores CV = 0.96878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# INITIALIZE VARIABLES\n",
    "cols = [c for c in train.columns if c not in ['id', 'target']]\n",
    "cols.remove('wheezy-copper-turtle-magic')\n",
    "oof = np.zeros(len(train))\n",
    "preds = np.zeros(len(test))\n",
    "\n",
    "# BUILD 512 SEPARATE MODELS\n",
    "for i in tqdm(range(512)):\n",
    "    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n",
    "    train2 = train[train['wheezy-copper-turtle-magic']==i]\n",
    "    test2 = test[test['wheezy-copper-turtle-magic']==i]\n",
    "    idx1 = train2.index; idx2 = test2.index\n",
    "    train2.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n",
    "    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n",
    "    train3 = sel.transform(train2[cols])\n",
    "    test3 = sel.transform(test2[cols])\n",
    "    \n",
    "    # STRATIFIED K-FOLD\n",
    "    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n",
    "    for train_index, test_index in skf.split(train3, train2['target']):\n",
    "        \n",
    "        # MODEL AND PREDICT WITH QDA\n",
    "        ms, ps = get_mean_cov(train3[train_index,:],train2.loc[train_index]['target'].values)\n",
    "        \n",
    "        gm = GaussianMixture(n_components=2, init_params='random', covariance_type='full', tol=0.001,reg_covar=0.001, max_iter=100, n_init=1,means_init=ms, precisions_init=ps)\n",
    "        gm.fit(np.concatenate([train3[train_index,:],test3],axis = 0))\n",
    "        oof[idx1[test_index]] = gm.predict_proba(train3[test_index,:])[:,0]\n",
    "        preds[idx2] += gm.predict_proba(test3)[:,0] / skf.n_splits\n",
    "\n",
    "        \n",
    "# PRINT CV AUC\n",
    "auc = roc_auc_score(train['target'],oof)\n",
    "print('QDA scores CV =',round(auc,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "sub['target'] = preds\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(preds,bins=100)\n",
    "plt.title('Final Test.csv predictions')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
